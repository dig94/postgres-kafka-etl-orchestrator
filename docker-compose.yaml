version: "3.8"

# x-airflow-common: &airflow-common
#   image: apache/airflow:2.9.3-python3.10
#   environment: &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#     AIRFLOW__CORE__FERNET_KEY: ''
#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#     AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#   volumes:
#     - ./airflow/dags:/opt/airflow/dags
#     - ./airflow/logs:/opt/airflow/logs
#     - ./airflow/plugins:/opt/airflow/plugins
#   depends_on:
#     redis:
#       condition: service_healthy
#     airflow-postgres:
#       condition: service_healthy

services:
  # ---------------------------------------------------------------------------
  # KAFKA + POSTGRES
  # ---------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.8.3
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_KRAFT_MODE: "true"
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka-data:/var/lib/kafka/data

    

  postgres:
    image: postgres:13
    container_name: source_postgres
    restart: always
    environment:
      POSTGRES_USER: remote_user
      POSTGRES_PASSWORD: strong_password
      POSTGRES_DB: test_db
    ports:
      - "5432:5432"

  iceberg-postgres:
    image: postgres:13
    container_name: iceberg-postgres
    restart: always
    environment:
      POSTGRES_USER: remote_user
      POSTGRES_PASSWORD: strong_password
      POSTGRES_DB: iceberg
    ports:
      - "5433:5432"

  # airflow-postgres:
  #   image: postgres:13
  #   container_name: airflow_db
  #   restart: always
  #   environment:
  #     POSTGRES_USER: airflow
  #     POSTGRES_PASSWORD: airflow
  #     POSTGRES_DB: airflow
  #   healthcheck:
  #     test: ["CMD", "pg_isready", "-U", "airflow"]
  #     interval: 10s
  #     retries: 5
  #   volumes:
  #     - airflow-db-volume:/var/lib/postgresql/data

  redis:
    image: redis:7.2-bookworm
    container_name: redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      retries: 50
      start_period: 30s
    restart: always

  # ---------------------------------------------------------------------------
  # AIRFLOW COMPONENTS
  # ---------------------------------------------------------------------------
  # airflow-webserver:
  #   <<: *airflow-common
  #   container_name: airflow-webserver
  #   command: webserver
  #   ports: ["8080:8080"]
  #   depends_on:
  #     - airflow-init

  # airflow-scheduler:
  #   <<: *airflow-common
  #   container_name: airflow-scheduler
  #   command: scheduler
  #   depends_on:
  #     - airflow-init

  # airflow-worker:
  #   <<: *airflow-common
  #   container_name: airflow-worker
  #   command: celery worker
  #   depends_on:
  #     - airflow-init

  # airflow-triggerer:
  #   <<: *airflow-common
  #   container_name: airflow-triggerer
  #   command: triggerer
  #   depends_on:
  #     - airflow-init

  # airflow-init:
  #   <<: *airflow-common
  #   container_name: airflow-init
  #   entrypoint: /bin/bash
  #   command:
  #     - -c
  #     - |
  #       airflow db init
  #       airflow users create --username admin --password admin \
  #         --firstname A --lastname U --role Admin --email admin@example.com
  #   user: "0:0"

  # ---------------------------------------------------------------------------
  # CONNECTOR SERVICE
  # ---------------------------------------------------------------------------
  connector:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: connector_service
    environment:
      POSTGRES_HOST: 35.207.193.219
      POSTGRES_PORT: 5432
      POSTGRES_DB: test_db
      POSTGRES_USER: remote_user
      POSTGRES_PASSWORD: strong_password
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS: miniouser
      MINIO_SECRET: miniopassword
      AWS_ACCESS_KEY_ID: miniouser
      AWS_SECRET_ACCESS_KEY: miniopassword
      AWS_REGION: us-east-1
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: extract_queue
      JDBC_JAR: /opt/jars/postgresql-42.2.27.jar
      SPARK_MASTER: local[*]
    depends_on:
      - postgres
      - kafka
      - redis
    command: ["main.py"]
    # volumes:
    #   - ./spark-jars:/opt/spark/jars
  
  # -------------------------------------------------------------------------
  # LOADER SERVICE (Spark + Iceberg)
  # -------------------------------------------------------------------------
  loader:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: loader_service
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: extract_queue
      PYTHONUNBUFFERED: "1"
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS: miniouser
      MINIO_SECRET: miniopassword
      AWS_ACCESS_KEY_ID: miniouser
      AWS_SECRET_ACCESS_KEY: miniopassword
      AWS_REGION: us-east-1
      
    depends_on:
      - kafka
      - minio
      - iceberg-postgres
    #entrypoint: ["python3"]
    #command: ["python3","-u","loader/kafka_consumer.py"]
    command: ["-u","loader/kafka_consumer.py"]
    
    # volumes:
    #   - ./spark-jars:/opt/spark/jars

  minio:
    image: minio/minio:RELEASE.2025-07-23T15-54-02Z
    container_name: minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: miniouser
      MINIO_ROOT_PASSWORD: miniopassword
    volumes:
    - minio_data:/data

  

  minio-init:
    image: minio/mc
    container_name: minio-init
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      mc alias set local http://minio:9000 miniouser miniopassword;
      mc mb -p local/raw;
      mc mb -p local/warehouse;
      exit 0;"

  trino:
    image: trinodb/trino:latest
    container_name: trino
    hostname: trino
    ports:
      - "8080:8080"
    depends_on:
      - minio
      - iceberg-postgres
    volumes:
      - ./Trino/etc/catalog:/etc/trino/catalog
    networks:
      - default


volumes:
  kafka-data:
  #airflow-db-volume:
  minio_data:
